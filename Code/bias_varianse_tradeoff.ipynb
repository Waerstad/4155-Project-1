{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e1dfdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# The function used to generate data, the Runge function\n",
    "def Runge_func(x):\n",
    "    return 1.0/(1 + 25*x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a18d0a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'polynomial_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m degree \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(poly_degrees)):\n\u001b[0;32m     24\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m2025\u001b[39m \u001b[38;5;241m+\u001b[39m degree)\n\u001b[1;32m---> 25\u001b[0m     X_train \u001b[38;5;241m=\u001b[39m polynomial_features(x_train, poly_degrees[degree], intercept\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     26\u001b[0m     X_test \u001b[38;5;241m=\u001b[39m polynomial_features(x_test, poly_degrees[degree], intercept\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# Scaling of the data\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'polynomial_features' is not defined"
     ]
    }
   ],
   "source": [
    "# Bias-variance-tradeoff, using bootstrap and OLS\n",
    "\n",
    "# Setting up dataset\n",
    "np.random.seed(2025)\n",
    "n = 1000\n",
    "x = np.linspace(-1, 1, n)\n",
    "y = Runge_func(x) + np.random.normal(0, 0.1, size=n)\n",
    "\n",
    "x = x.reshape(-1,1)\n",
    "y = y.reshape(-1,1)\n",
    "\n",
    "bootstraps = 10\n",
    "\n",
    "max_degree = 35\n",
    "poly_degrees = np.arange(1, max_degree + 1, 1)\n",
    "\n",
    "biases = np.zeros(max_degree)\n",
    "variances = np.zeros(max_degree)\n",
    "MSEs = np.zeros(max_degree)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "for degree in range(len(poly_degrees)):\n",
    "    np.random.seed(2025 + degree)\n",
    "    X_train = polynomial_features(x_train, poly_degrees[degree], intercept=True)\n",
    "    X_test = polynomial_features(x_test, poly_degrees[degree], intercept=True)\n",
    "\n",
    "    # Scaling of the data\n",
    "    scaler = StandardScaler()   # initialize scaler method\n",
    "    scaler.fit(X_train)               # only base the scaling on X_train, to prevent data leakage from X_test \n",
    "    X_train_s = scaler.transform(X_train)\n",
    "    X_test_s = scaler.transform(X_test)\n",
    "    \n",
    "    # predictions are later filled with predictions made from X_test (feature matrix), constructed from x_test, \n",
    "    # so got to have same length\n",
    "    predictions = np.zeros([bootstraps, len(x_test)])\n",
    "\n",
    "    for b in range(bootstraps):\n",
    "        np.random.seed(1025 + b)\n",
    "        # For each bootstrap sample of X_train and Y_train, we train model, predict on X_test\n",
    "        # Later comparing against the un-touched y_test\n",
    "        X_train_resampled, y_train_resampled = resample(X_train_s, y_train)\n",
    "\n",
    "        # Training the model on training data\n",
    "        beta = OLS_parameters(X_train_resampled, y_train_resampled)\n",
    "        # Making the prediction on test data\n",
    "        pred = X_test_s @ beta\n",
    "        predictions[b,:] = pred.ravel()\n",
    "\n",
    "    # We take the true values or target, as the un-tough values in the y_test split\n",
    "    # The predicted y values, lives in the predictions matrix, where each row is a sample of values,\n",
    "    # and each column corresponding to a one y point across bootstrap samples \n",
    "    biases[degree] = np.mean((y_test.T - np.mean(predictions, axis=0))**2)\n",
    "\n",
    "    # Var(prediction) is the mean of the flatend matrix, over all samples\n",
    "    variances[degree] = np.mean((predictions - np.mean(predictions, axis=0))**2)\n",
    "\n",
    "    # For the MSE, we take difference of each y point per bootstrap sample, making y_test a row vector\n",
    "    # then squaring, before taking the mean over the flattened matrix\n",
    "    \n",
    "    MSEs[degree] = np.mean(np.mean((predictions - y_test.T)**2, axis=1), axis=0)\n",
    "\n",
    "plt.plot(poly_degrees, biases, 'o-', label=\"Bias\")\n",
    "plt.plot(poly_degrees, variances, 'o-', label=\"Variance\")\n",
    "plt.plot(poly_degrees, MSEs, 'o-', label=\"MSE\")\n",
    "#plt.title(f\"Bias-variance tradeoff, for {n} data points and {bootstraps} bootstraps\")\n",
    "plt.xlabel(\"Polynomial degree\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.title(\"OLS\")\n",
    "plt.legend()\n",
    "plt.savefig(\"bias_var_tradeoff.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
